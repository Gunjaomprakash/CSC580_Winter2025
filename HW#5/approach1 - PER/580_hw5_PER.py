# -*- coding: utf-8 -*-
"""tryonOm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14k9xAoni2gEENdyEXmDJqBjggARNAt5w

Om Prakash Gunja

## 2025 HW\#5 Atari Breakout using DQN -- Starter Code

## 1. CoLab Setup
"""

## Code piece to mount my Google Drive
from google.colab import drive
drive.mount("/content/drive") # my Google Drive root directory will be mapped here

# Change the working directory to your own work directory (where the code file is).
import os
thisdir = '/content/drive/My Drive/CSC580_Winter2025/HW5_2'
os.chdir(thisdir)

# # Ensure the files are there (in the folder)
!pwd

"""### 2. Install libraries related to graphics and Gymnasium

"""

!apt-get update
!apt install xvfb -y
!pip install pyvirtualdisplay
!pip install piglet


from pyvirtualdisplay import Display
display = Display(visible=0, size=(1400, 900))
display.start()

import base64
import imageio
import IPython
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
from PIL import Image, ImageOps
import pyvirtualdisplay

# Commented out IPython magic to ensure Python compatibility.
# % to ensure the libraries are installed in the current environment
# %pip install gymnasium[atari]
# %pip install gymnasium[accept-rom-license]
# %pip install ale-py     # critical for rendering Atari screens

# First import the ALE
from ale_py import ALEInterface

ale = ALEInterface()

"""### 3. Preliminary step to inspect states and rendering

"""

import gymnasium as gym

env = gym.make("ALE/Breakout-v5", render_mode='rgb_array', full_action_space=False)

# Various information about the environment
print ("action_space={}, observation_space={}".format(env.action_space.n, env.observation_space.shape))

observation = env.reset() # initial state/observation returned
#print (observation[0][180])
env.step(1) ## FIRE!

#env.render()  # returns an array of (210, 160, 3), each in the range [0-255]
Image.fromarray(env.render())

"""## 4. Install tensorflow

"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install tensorflow

import tensorflow as tf
from tensorflow import keras
tf.__version__

"""### (\*) Image/frame preprocessing -- cropping, resizing and grayscale

[from https://towardsdatascience.com/getting-an-ai-to-play-atari-pong-with-deep-reinforcement-learning-47b0c56e78ae]<br>
Currently, the frames received from openai are much larger than we need, with a much higher resolution than we need. We donâ€™t need any of the white space at the bottom, or any pixel above the white stripe at the top.

So we first crop the image so that only the important area is displayed. Next, we convert the image to grayscale if desired. Finally we convert the image datatype to np.uint8 (to save space than regular int).
"""

import numpy as np

def crop_Atari_frame(frame):
    return frame[30:-12,5:-4]  # crop out irrelevant parts in the frame/image

def resize_and_gray(frame, newsize, gray = False):
    """ converts the frame/image array, possibly reduce to the gray scale,
        then resizes to the new size.
    """
    frame = crop_Atari_frame(frame)       # first crop the frame/array
    im = Image.fromarray(np.uint8(frame)) # convert the array to a PIL image
    if gray:
      im = ImageOps.grayscale(im)  # convert to grayscale using PIL ImageOps

    im = im.resize(newsize)  # resize the PIL image to the desired size
    return np.array(im, dtype = np.uint8) # convert back to an np array

import cv2
def crop_Atari_frame_cv2(frame):
        # Define cropping region
    x_start = 5  # Starting x-coordinate
    y_start = 30  # Starting y-coordinate
    width = frame.shape[1] - 10  # Width of the cropped region
    height = frame.shape[0] - 42  # Height of the cropped region
    # Crop using slicing
    cropped_frame = frame[y_start:y_start + height, x_start:x_start + width]
    return cropped_frame

def resize_and_gray_cv2(frame, newsize, gray=False):
    frame = crop_Atari_frame_cv2(frame)
    if gray:
        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)
    return cv2.resize(frame, newsize, interpolation=cv2.INTER_AREA)

import gymnasium as gym
import matplotlib.pyplot as plt

env = gym.make("ALE/Breakout-v5", render_mode='rgb_array', full_action_space=False)
state = env.reset()
env.step(1)

newsize = (84, 84)  # reduced, new size!!

## convert the frame to a smaller color image
first_frame = resize_and_gray_cv2(state[0], newsize)  # new size, but still in color
print ("Image shape: {}".format(first_frame.shape))
plt.imshow(first_frame)

## convert the frame to a smaller grayscale image
first_frame = resize_and_gray_cv2(state[0], newsize, True) # new size, in grayscale
print ("Image shape: {}".format(first_frame.shape))
plt.imshow(first_frame)

"""## 5. Define QNetwork and Memory

"""

from keras import layers
from collections import deque
import random

class QNetwork:
    def __init__(self, input_shape=(84, 84, 3), action_size=4, hidden_size=126,
                 learning_rate = 0.001):
        # First fix up input_shape in case it is a 2D grayscale image
        if len(input_shape) < 3:
            input_shape = (input_shape[0], input_shape[1], 1)
        print ('input_shape = {}'.format(input_shape))

        # Model in the original Deepmind DQN paper (but reduced to assume one frame)
        self.model = keras.Sequential()
        self.model.add(layers.Conv2D(32, 8, strides=4, activation="relu",
                                    input_shape=input_shape))
        self.model.add(layers.Conv2D(64, 4, strides=2, activation="relu"))
        self.model.add(layers.Conv2D(64, 3, strides=1, activation="relu"))
        self.model.add(layers.Flatten())
        self.model.add(layers.Dense(hidden_size, activation="relu"))
        self.model.add(layers.Dense(action_size, activation="linear"))

        # other compile parameters
        self.optimizer = keras.optimizers.RMSprop(learning_rate=learning_rate)
        self.model.compile(loss=keras.losses.Huber(), optimizer=self.optimizer)


class Memory():
    """ Replay memory """
    def __init__(self, max_size=1000):
        self.buffer = deque(maxlen=max_size)

    def add(self, experience):
        self.buffer.append(experience)

    def sample(self, batch_size):
        idx = np.random.choice(np.arange(len(self.buffer)),
                               size=batch_size,
                               replace=False)
        return [self.buffer[ii] for ii in idx]

    def __len__(self):
        return len(self.buffer)

class PrioritizedReplayBuffer:
    def __init__(self, capacity, alpha=0.6):
        self.capacity = capacity
        self.buffer = []
        self.priorities = []
        self.alpha = alpha  # Controls how much prioritization matters (0 = uniform, 1 = full prioritization)

    def add(self, experience, td_error):
        priority = (abs(td_error) + 1e-5) ** self.alpha  # Compute priority
        self.buffer.append(experience)
        self.priorities.append(priority)

        if len(self.buffer) > self.capacity:
            del self.buffer[0]
            del self.priorities[0]

    def sample(self, batch_size):
        if len(self.buffer) == 0:
            return []

        # Convert priorities to probabilities
        priorities = np.array(self.priorities) / sum(self.priorities)
        indices = np.random.default_rng().choice(len(self.buffer), batch_size, replace=False, p=priorities)

        batch = [self.buffer[i] for i in indices]
        return batch, indices

    def update_priority(self, indices, td_errors):
        for i, td_error in zip(indices, td_errors):
            self.priorities[i] = (abs(td_error) + 1e-5) ** self.alpha

    def __len__(self): # Add this method to the class
        return len(self.buffer)

"""### 6. Fill the Memory with episode steps

"""

###################################
## Populate the experience memory
###################################

# Newly create the Atari Breakout game environment
env = gym.make("ALE/Breakout-v5", render_mode='rgb_array', full_action_space=False)
newsize = (84, 84)
grayscale = False

# Memory parameters
memory_size = 10000            # memory capacity
batch_size = 16                # experience mini-batch size
pretrain_length = batch_size   # number experiences to pretrain the memory

INIT_MEMORY = 100

# memory = Memory(max_size=memory_size)
memory = PrioritizedReplayBuffer(capacity=10000)  # Choose capacity based on your GPU memory
#----------------
print ("* Populate the experience memory with {} steps *".format(INIT_MEMORY))

# Initialize the simulation
observation = env.reset()
state = resize_and_gray_cv2(observation[0], newsize)

# Make a bunch of random actions and store the experiences
for _ in range(INIT_MEMORY):
    # render commented since it doesn't work well on CoLab without video
    #env.render()

    # Make a/one random action
    action = env.action_space.sample()
    # take the action
    next_state, reward, done, info, _ = env.step(action)

    # resize the image of next_state
    next_state = resize_and_gray_cv2(next_state, newsize, grayscale)

    # Add experience (a four-tuple) to memory.
    # (**) Keras predict processes a batch, so when you're using a single
    #  image, you need to add it to a list (by expanding the row dimension):
    memory.add((np.expand_dims(state, axis=0), action, reward,
                np.expand_dims(next_state, axis=0), done), td_error = 0)

    if done:
        # Start a new episode
        env.reset()
        # Take one random step to get started
        state, reward, done, info = env.step(env.action_space.sample())
        state = resize_and_gray_cv2(state, newsize, grayscale)
    else:
        # go to the next state
        state = next_state

print ("Done!")

"""## 7. Create DQN networks ('model_pred', which utilizes 'model_target') as Agent

"""

# The first model makes the predictions for Q-values which are used to
# make a action.
image_shape = (memory.buffer[0][0][0]).shape
num_actions = env.action_space.n

model_pred = QNetwork(input_shape = image_shape, action_size = num_actions,
                      hidden_size=126)

# Build a target model for the prediction of future rewards.
# The weights of a target model get updated every 1000 steps thus when the
# loss between the Q-values is calculated the target Q-value is stable.
model_target = QNetwork(input_shape = image_shape, action_size = num_actions,
                        hidden_size=126)

"""Some code to inspect memory, states etc.

"""

frame = memory.sample(1)
frame_state,indices = frame # Unpack the tuple returned by sample
frame_state = frame_state[0][0] # Access the NumPy array within the tuple
print(frame_state.shape)    # still in a batch of one

plt.imshow(frame_state[0])  # show only the image

"""(\*\*) Convert the image into range \[0, 1\], then call 'predict' in model_pred.model

"""

# convert values of the image into range [0, 1]
frame_state = frame_state / 255.0


predictions = model_pred.model.predict(frame_state)
print (predictions[0])  # [0] because there is only one result in predictions

"""# 8. Your Training Code goes here!!!

"""

from logging import log
import csv
import numpy as np
import tensorflow as tf
from tensorflow import keras
import os


log_file = 'training_log.csv'
with open(log_file, 'w', newline='') as csvfile:
    writer = csv.writer(csvfile)
    writer.writerow(['Episode', 'Reward', 'Epsilon', 'Loss', 'Steps', 'Batch Size'])  # Added 'Batch Size' column


# Hyperparameters
gamma = 0.99  # Discount factor
epsilon = 1.0  # Exploration rate
epsilon_min = 0.001
epsilon_decay = 0.9995
learning_rate = 0.001
update_target_every = 500
batch_size = 12
max_episodes = 3000

# --- Dynamic Batch Size Parameters ---
minimum_batch_size = 12  # Set a minimum batch size
maximum_batch_size = 64  # Set a maximum batch size
initial_loss = 0.5  # Initial loss (or a reasonable value)
loss_threshold = 2
# -------------------------------------

# --- Network Update Frequency ---
update_frequency = 32 # Update every 32 steps
# --------------------------------
# Initialize update_counter
update_counter = 0  # Initialize the counter

log_interval = 20
log_if_max = False


# Optimization
optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
loss_fn = keras.losses.Huber()

# Checkpoint saving
checkpoint_dir = './training_checkpoints'
checkpoint_prefix = os.path.join(checkpoint_dir, "ckpt")
checkpoint = tf.train.Checkpoint(optimizer=optimizer,
                                 model_pred=model_pred.model,
                                 model_target=model_target.model)

# Define train_step outside the loop
@tf.function(reduce_retracing=True, input_signature=[
    tf.TensorSpec(shape=[None, 84, 84, 3], dtype=tf.float32),  # states
    tf.TensorSpec(shape=[None], dtype=tf.int32),              # actions
    tf.TensorSpec(shape=[None], dtype=tf.float32)             # targets
])
def train_step(states, actions, targets):
    with tf.GradientTape() as tape:
        q_values = model_pred.model(states)
        q_values_selected = tf.reduce_sum(tf.multiply(q_values, tf.one_hot(actions, num_actions)), axis=1)
        loss = loss_fn(targets, q_values_selected)
    grads = tape.gradient(loss, model_pred.model.trainable_variables)
    grads = [tf.clip_by_value(g, -1.0, 1.0) for g in grads]  # Example clipping to [-1, 1]
    optimizer.apply_gradients(zip(grads, model_pred.model.trainable_variables))
    return loss

# Training loop
max_reward = -float('inf')
for episode in range(max_episodes):
    state = env.reset()[0]
    state = resize_and_gray_cv2(state, newsize, grayscale)
    state = np.expand_dims(state, axis=0)
    total_reward = 0
    loss_list = []  # Track losses
    step_count = 0

    for step in range(10000): # Maximum steps per episode
        step_count += 1
        # Epsilon-greedy action selection
        if np.random.rand() <= epsilon:
            action = env.action_space.sample()
        else:
            q_values = model_pred.model.predict_on_batch(state / 255.0)
            action = np.argmax(q_values[0])

        accumulated_reward = 0
        for _ in range(4):  # Skip 4 frames (including the first one)
            next_state, reward, done, _, _ = env.step(action)
            accumulated_reward += reward
            if done:
                break

        next_state = resize_and_gray_cv2(next_state, newsize, grayscale)
        next_state = np.expand_dims(next_state, axis=0)
        total_reward += accumulated_reward  # Add accumulated reward

        # memory.add((state, action, accumulated_reward, next_state, done))
        q_values = model_pred.model.predict_on_batch(state / 255.0)
        target_q_values = model_target.model.predict_on_batch(next_state / 255.0)
        best_action = np.argmax(target_q_values[0])  # Use target network

        td_error = accumulated_reward + gamma * target_q_values[0][best_action] - q_values[0][action]
        memory.add((state, action, accumulated_reward, next_state, done), td_error)



        if len(memory) >= batch_size: # changed to check memory length
            minibatch, indices = memory.sample(batch_size)

            states = np.array([i[0] for i in minibatch]) / 255.0
            states = states.squeeze(axis=1)
            actions = np.array([i[1] for i in minibatch])
            rewards = np.array([i[2] for i in minibatch])
            next_states = np.array([i[3] for i in minibatch]) / 255.0
            next_states = next_states.squeeze(axis=1)
            dones = np.array([i[4] for i in minibatch])

            # Calculate target Q-values with parallelism
            target_q_values = model_target.model.predict_on_batch(next_states)
            targets = rewards + gamma * np.max(target_q_values, axis=1) * (1 - dones.astype(np.float32))


            loss = train_step(states, actions, targets)
            loss_list.append(loss.numpy())  # save loss for current batch

            q_values_selected = model_pred.model.predict_on_batch(states)
            q_values_selected = np.take_along_axis(q_values_selected, actions[:, None], axis=1).squeeze()

            td_errors = rewards + gamma * np.max(target_q_values, axis=1) * (1 - dones) - q_values_selected

            memory.update_priority(indices, td_errors)

            average_loss = np.mean(loss_list)

            if average_loss > loss_threshold:
                batch_size = max(batch_size // 2, minimum_batch_size)  # Reduce batch size, with a minimum
            else:
                batch_size = min(batch_size * 2, maximum_batch_size)  # Increase batch size, with a maximum
            # Update loss_threshold (e.g., using a moving average)
            loss_threshold = 0.9 * loss_threshold + 0.1 * average_loss

        # --- Network Update Frequency ---
        if update_counter % update_frequency == 0 and len(memory) >= batch_size:  # Update prediction network
            model_target.model.set_weights(model_pred.model.get_weights())
            update_counter = 0  # Reset update counter
        # --------------------------------

        update_counter += 1  # Increment update counter
        state = next_state
        if done:
            break

    # Update target network periodically
    if episode % update_target_every == 0:
        model_target.model.set_weights(model_pred.model.get_weights())

    # Decay exploration rate
    epsilon = max(epsilon * epsilon_decay, epsilon_min)

    # Checkpoint saving based on max reward
    if total_reward > max_reward:
        log_if_max = True
        max_reward = total_reward
        checkpoint.save(file_prefix=checkpoint_prefix)
        print("Max Reward Updated, Checkpoint saved.")
    print(f"Episode {episode} | Total Reward: {total_reward} | Steps: {step_count} | Epsilon: {epsilon:.4f}")

    if episode % log_interval == 0 or log_if_max:
        with open(log_file, 'a', newline='') as csvfile:
          writer = csv.writer(csvfile)
          writer.writerow([episode, total_reward, epsilon, np.mean(loss_list), step_count, batch_size])  # Added batch_size
        log_if_max = False

"""Store

"""

import tensorflow as tf
import os

# Directory to store the models
model_dir = 'saved_models'  # Create this directory if it doesn't exist
os.makedirs(model_dir, exist_ok=True)

# Save model_pred using tf.saved_model, adding .keras extension
model_pred_path = os.path.join(model_dir, 'model_pred.keras')
model_pred.model.save(model_pred_path)

# Save model_target using tf.saved_model, adding .keras extension
model_target_path = os.path.join(model_dir, 'model_target.keras')
model_target.model.save(model_target_path)

print(f"Models saved to directory: {model_dir}")

"""load

"""

import tensorflow as tf
import os

# Directory where models are stored
model_dir = 'saved_models'

# Load model_pred
model_pred_path = os.path.join(model_dir, 'model_pred.keras')
loaded_model_pred = tf.keras.models.load_model(model_pred_path)

# Load model_target
model_target_path = os.path.join(model_dir, 'model_target.keras')
loaded_model_target = tf.keras.models.load_model(model_target_path)

print("Models loaded successfully!")

"""## 9. Try out the learned policy!!

Utility functions related to video

"""

from gym import logger as gymlogger
from gym.wrappers.record_video import RecordVideo
gymlogger.set_level(40) #error only
import glob
import io
from IPython import display as ipythondisplay
from IPython.display import HTML

"""
Utility functions to enable video recording of gym environment and displaying it
To enable video, just do "env = wrap_env(env)""
"""

def show_video():
  mp4list = glob.glob('video/*.mp4')
  if len(mp4list) > 0:
    mp4 = mp4list[0]
    video = io.open(mp4, 'r+b').read()
    encoded = base64.b64encode(video)
    ipythondisplay.display(HTML(data='''<video alt="test" autoplay
                loop controls style="height: 400px;">
                <source src="data:video/mp4;base64,{0}" type="video/mp4" />
             </video>'''.format(encoded.decode('ascii'))))
  else:
    print("Could not find video")

def wrap_env(env):
  env = RecordVideo(env, './video',  episode_trigger = lambda episode_number: True)
  return env

"""(\*\*)

"""

# import tensorflow as tf

# # Load the saved model
# checkpoint = tf.train.Checkpoint(optimizer=optimizer,
#                                  model_pred=model_pred.model,
#                                  model_target=model_target.model)
# checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))

# First wrap the enviroment in a video-recording environment
env = wrap_env(gym.make("ALE/Breakout-v5", render_mode='rgb_array', full_action_space=False)) # Recreate environment with render_mode


num_episodes = 10
total_score = 0

newsize = (84, 84)

for _ in range(num_episodes):
    # for each episode
    observation = env.reset()
    state = resize_and_gray_cv2(observation[0], newsize)
    done = False
    score = 0

    #while not done:
    for i in range(50):
        if i % 4 == 0:
          env.render()

        # Get action from the model_pred
        state = state / 255.0
        predictions = model_pred.model.predict(np.expand_dims(state, axis=0), verbose=0)[0]
        max_indices = np.where(predictions == np.max(predictions))[0]  # indices of max values
        action = np.random.choice(max_indices)  # break the ties randomly

        #action = np.argmax()

        # take the action
        next_state, reward, done, info = env.step(action)
        next_state = resize_and_gray_cv2(next_state, newsize, grayscale)

        score += reward
        state = next_state

env.close()
show_video()

